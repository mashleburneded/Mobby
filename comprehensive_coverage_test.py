#!/usr/bin/env python3
"""
Comprehensive Coverage Test Suite for M√∂bius AI Assistant
Tests 50+ different commands and scenarios to identify all bugs and issues
"""

import asyncio
import json
import logging
import os
import sys
import time
import traceback
from datetime import datetime
from typing import Dict, List, Any, Optional
from unittest.mock import Mock, AsyncMock, patch

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensiveCoverageTest:
    """Comprehensive test suite for full coverage testing"""
    
    def __init__(self):
        self.test_results = []
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
        self.error_tests = 0
        self.start_time = datetime.now()
        
        # Test categories
        self.test_categories = {
            'basic_commands': [],
            'natural_language': [],
            'crypto_queries': [],
            'defi_protocols': [],
            'portfolio_management': [],
            'alerts_and_notifications': [],
            'ai_provider_switching': [],
            'error_handling': [],
            'edge_cases': [],
            'performance_tests': [],
            'security_tests': [],
            'integration_tests': []
        }
        
    async def run_all_tests(self):
        """Run all comprehensive tests"""
        logger.info("üöÄ Starting Comprehensive Coverage Test Suite")
        logger.info(f"Test started at: {self.start_time}")
        
        # Test categories in order
        test_methods = [
            self.test_basic_commands,
            self.test_natural_language_processing,
            self.test_crypto_queries,
            self.test_defi_protocols,
            self.test_portfolio_management,
            self.test_alerts_and_notifications,
            self.test_ai_provider_switching,
            self.test_error_handling,
            self.test_edge_cases,
            self.test_performance,
            self.test_security,
            self.test_integrations
        ]
        
        for test_method in test_methods:
            try:
                await test_method()
            except Exception as e:
                logger.error(f"Test category {test_method.__name__} failed: {e}")
                self.record_test_result(test_method.__name__, False, str(e), "CATEGORY_FAILURE")
        
        # Generate final report
        await self.generate_final_report()
        
    async def test_basic_commands(self):
        """Test basic bot commands"""
        logger.info("üìã Testing Basic Commands")
        
        basic_commands = [
            "/start",
            "/help", 
            "/status",
            "/version",
            "/settings",
            "/profile",
            "/subscription",
            "/upgrade",
            "/cancel",
            "/reset"
        ]
        
        for command in basic_commands:
            await self.test_command_execution(command, "basic_commands")
            
    async def test_natural_language_processing(self):
        """Test natural language understanding and processing"""
        logger.info("üß† Testing Natural Language Processing")
        
        nl_queries = [
            "What's the price of Bitcoin?",
            "Show me Ethereum price",
            "How much is BTC worth?",
            "Tell me about Uniswap",
            "What are the best yield farming opportunities?",
            "Check my portfolio",
            "Set an alert for ETH at $3000",
            "Remove all my alerts",
            "What's happening in DeFi today?",
            "Explain what is liquidity mining",
            "How do I stake my tokens?",
            "What's the TVL of Aave?",
            "Show me top DeFi protocols",
            "What's the gas price on Ethereum?",
            "Help me understand impermanent loss",
            "What are the risks of yield farming?",
            "How do I bridge tokens to Polygon?",
            "What's the best DEX for trading?",
            "Show me arbitrage opportunities",
            "What's the market cap of Solana?",
            "How do I provide liquidity on Uniswap?",
            "What's the APY on Compound?",
            "Show me cross-chain bridges",
            "What's the latest DeFi news?",
            "How do I calculate slippage?"
        ]
        
        for query in nl_queries:
            await self.test_natural_language_query(query, "natural_language")
            
    async def test_crypto_queries(self):
        """Test cryptocurrency-related queries"""
        logger.info("üí∞ Testing Crypto Queries")
        
        crypto_queries = [
            "BTC price",
            "ETH price", 
            "SOL price",
            "MATIC price",
            "AVAX price",
            "LINK price",
            "UNI price",
            "AAVE price",
            "COMP price",
            "MKR price",
            "price of bitcoin",
            "ethereum price now",
            "solana market cap",
            "polygon volume",
            "avalanche price chart",
            "chainlink price prediction",
            "uniswap token price",
            "aave protocol stats",
            "compound interest rates",
            "maker dao governance"
        ]
        
        for query in crypto_queries:
            await self.test_crypto_price_query(query, "crypto_queries")
            
    async def test_defi_protocols(self):
        """Test DeFi protocol queries"""
        logger.info("üè¶ Testing DeFi Protocols")
        
        defi_queries = [
            "Uniswap V3 stats",
            "Aave lending rates",
            "Compound borrowing rates", 
            "MakerDAO collateral ratio",
            "Curve pool APY",
            "Balancer pool weights",
            "SushiSwap volume",
            "PancakeSwap farms",
            "Yearn vault strategies",
            "Convex rewards",
            "Lido staking APR",
            "Rocket Pool validators",
            "Frax protocol stats",
            "Olympus DAO bonds",
            "Tokemak reactors",
            "Alchemix self-repaying loans",
            "Abracadabra cauldrons",
            "Spell token staking",
            "CVX token utility",
            "CRV token economics"
        ]
        
        for query in defi_queries:
            await self.test_defi_protocol_query(query, "defi_protocols")
            
    async def test_portfolio_management(self):
        """Test portfolio management features"""
        logger.info("üìä Testing Portfolio Management")
        
        portfolio_commands = [
            "show my portfolio",
            "add 1 BTC to portfolio",
            "remove ETH from portfolio", 
            "portfolio balance",
            "portfolio performance",
            "rebalance portfolio",
            "portfolio allocation",
            "portfolio risk analysis",
            "portfolio yield",
            "portfolio history",
            "export portfolio",
            "import portfolio",
            "portfolio alerts",
            "portfolio summary",
            "portfolio optimization"
        ]
        
        for command in portfolio_commands:
            await self.test_portfolio_command(command, "portfolio_management")
            
    async def test_alerts_and_notifications(self):
        """Test alerts and notification system"""
        logger.info("üîî Testing Alerts and Notifications")
        
        alert_commands = [
            "set price alert BTC 50000",
            "set price alert ETH 3000",
            "remove alert BTC",
            "list my alerts",
            "disable all alerts",
            "enable alerts",
            "alert when gas < 20 gwei",
            "alert when TVL changes 10%",
            "set yield alert > 20% APY",
            "alert me about new pools",
            "set liquidation alert",
            "alert on large transactions",
            "set governance alert",
            "alert on protocol updates",
            "set market alert"
        ]
        
        for command in alert_commands:
            await self.test_alert_command(command, "alerts_and_notifications")
            
    async def test_ai_provider_switching(self):
        """Test AI provider switching functionality"""
        logger.info("ü§ñ Testing AI Provider Switching")
        
        ai_commands = [
            "switch to groq",
            "switch to gemini", 
            "switch to openai",
            "list ai providers",
            "current ai provider",
            "test ai provider groq",
            "test ai provider gemini",
            "ai provider status",
            "ai provider info",
            "reset ai provider"
        ]
        
        for command in ai_commands:
            await self.test_ai_provider_command(command, "ai_provider_switching")
            
    async def test_error_handling(self):
        """Test error handling and recovery"""
        logger.info("‚ö†Ô∏è Testing Error Handling")
        
        error_scenarios = [
            "invalid command xyz123",
            "price of nonexistent_token",
            "set alert with invalid syntax",
            "portfolio add invalid amount",
            "switch to invalid_ai_provider",
            "query empty string",
            "very long query " + "x" * 1000,
            "query with special chars !@#$%^&*()",
            "sql injection attempt'; DROP TABLE users; --",
            "xss attempt <script>alert('xss')</script>",
            "command with null bytes \x00",
            "unicode test üöÄüí∞üìàüî•",
            "malformed json {invalid: json}",
            "extremely nested query " + "(" * 100 + ")" * 100,
            "binary data \x89PNG\r\n\x1a\n"
        ]
        
        for scenario in error_scenarios:
            await self.test_error_scenario(scenario, "error_handling")
            
    async def test_edge_cases(self):
        """Test edge cases and boundary conditions"""
        logger.info("üéØ Testing Edge Cases")
        
        edge_cases = [
            "",  # Empty input
            " ",  # Whitespace only
            "\n\t\r",  # Various whitespace
            "a",  # Single character
            "A" * 10000,  # Very long input
            "price",  # Incomplete command
            "BTC",  # Just symbol
            "123456789",  # Just numbers
            "!@#$%^&*()",  # Special characters only
            "price BTC ETH SOL",  # Multiple symbols
            "set alert BTC 0",  # Zero price alert
            "set alert BTC -100",  # Negative price alert
            "set alert BTC 999999999",  # Extremely high price
            "portfolio add 0 BTC",  # Zero amount
            "portfolio add -1 ETH",  # Negative amount
            "portfolio add 999999999 SOL",  # Huge amount
            "switch to " + "x" * 100,  # Long provider name
            "help help help help help",  # Repeated commands
            "/start /help /status",  # Multiple commands
            "price of bitcoin ethereum solana polygon avalanche chainlink uniswap aave compound maker"  # Many tokens
        ]
        
        for case in edge_cases:
            await self.test_edge_case(case, "edge_cases")
            
    async def test_performance(self):
        """Test performance and response times"""
        logger.info("‚ö° Testing Performance")
        
        performance_tests = [
            ("simple_query", "BTC price"),
            ("complex_query", "What's the best yield farming strategy for maximizing returns while minimizing impermanent loss?"),
            ("data_heavy", "Show me all DeFi protocols with TVL > $1B and APY > 10%"),
            ("concurrent_simple", "ETH price"),
            ("concurrent_complex", "Analyze the correlation between Bitcoin price and DeFi TVL"),
            ("rapid_fire", "price"),
            ("memory_intensive", "portfolio analysis with historical data"),
            ("api_intensive", "real-time market data for top 100 tokens"),
            ("ai_intensive", "explain quantum computing impact on blockchain"),
            ("mixed_load", "BTC price, ETH price, portfolio check, set alert")
        ]
        
        for test_name, query in performance_tests:
            await self.test_performance_scenario(test_name, query, "performance_tests")
            
    async def test_security(self):
        """Test security measures and vulnerabilities"""
        logger.info("üîí Testing Security")
        
        security_tests = [
            ("sql_injection", "'; DROP TABLE users; --"),
            ("xss_attempt", "<script>alert('xss')</script>"),
            ("command_injection", "; rm -rf /"),
            ("path_traversal", "../../../etc/passwd"),
            ("buffer_overflow", "A" * 100000),
            ("format_string", "%s%s%s%s%s"),
            ("null_byte", "test\x00.txt"),
            ("unicode_bypass", "ÔºúscriptÔºûalert('xss')Ôºú/scriptÔºû"),
            ("encoding_bypass", "%3Cscript%3Ealert('xss')%3C/script%3E"),
            ("ldap_injection", "*()|&'"),
            ("nosql_injection", "{'$ne': null}"),
            ("xml_injection", "<?xml version='1.0'?><!DOCTYPE root [<!ENTITY test SYSTEM 'file:///etc/passwd'>]><root>&test;</root>"),
            ("regex_dos", "a" * 1000 + "!"),
            ("zip_bomb", "PK\x03\x04" + "\x00" * 1000),
            ("billion_laughs", "<!DOCTYPE lolz [<!ENTITY lol 'lol'><!ENTITY lol2 '&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;'>]><lolz>&lol2;</lolz>")
        ]
        
        for test_name, payload in security_tests:
            await self.test_security_scenario(test_name, payload, "security_tests")
            
    async def test_integrations(self):
        """Test external integrations and APIs"""
        logger.info("üîó Testing Integrations")
        
        integration_tests = [
            ("coingecko_api", "BTC price from CoinGecko"),
            ("defillama_api", "Uniswap TVL from DeFiLlama"),
            ("telegram_api", "send message test"),
            ("database_connection", "user data storage"),
            ("redis_cache", "cache performance"),
            ("mcp_integration", "MCP server connection"),
            ("ai_provider_groq", "Groq API test"),
            ("ai_provider_gemini", "Gemini API test"),
            ("blockchain_rpc", "Ethereum RPC call"),
            ("websocket_connection", "real-time data stream"),
            ("file_system", "data persistence"),
            ("environment_vars", "configuration loading"),
            ("logging_system", "error logging"),
            ("monitoring", "health checks"),
            ("backup_system", "data backup")
        ]
        
        for test_name, description in integration_tests:
            await self.test_integration_scenario(test_name, description, "integration_tests")
            
    async def test_command_execution(self, command: str, category: str):
        """Test basic command execution"""
        test_name = f"command_{command.replace('/', '')}"
        
        try:
            # Mock Telegram update and context
            update = self.create_mock_update(command)
            context = self.create_mock_context()
            
            # Import and test command handlers
            from main import safe_command
            
            # Test if command handler exists and executes
            start_time = time.time()
            
            # Simulate command execution
            result = await self.simulate_command_execution(command, update, context)
            
            execution_time = time.time() - start_time
            
            if result.get('success', False):
                self.record_test_result(test_name, True, f"Command executed successfully in {execution_time:.3f}s", category)
            else:
                self.record_test_result(test_name, False, result.get('error', 'Unknown error'), category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Exception: {str(e)}", category, traceback.format_exc())
            
    async def test_natural_language_query(self, query: str, category: str):
        """Test natural language processing"""
        test_name = f"nl_{hash(query) % 10000}"
        
        try:
            # Test natural language processing
            from enhanced_natural_language import process_natural_language
            from enhanced_intent_system import analyze_user_intent_enhanced
            
            start_time = time.time()
            
            # Test intent analysis
            intent_analysis = await analyze_user_intent_enhanced(query, user_id=12345)
            
            # Test response generation
            if intent_analysis:
                from enhanced_response_handler import EnhancedResponseHandler
                handler = EnhancedResponseHandler()
                response = await handler.handle_intent(intent_analysis, query, 12345, {})
                
                execution_time = time.time() - start_time
                
                if response and response.get('message'):
                    self.record_test_result(test_name, True, f"NL processed successfully in {execution_time:.3f}s", category)
                else:
                    self.record_test_result(test_name, False, "No response generated", category)
            else:
                self.record_test_result(test_name, False, "Intent analysis failed", category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Exception: {str(e)}", category, traceback.format_exc())
            
    async def test_crypto_price_query(self, query: str, category: str):
        """Test cryptocurrency price queries"""
        test_name = f"crypto_{hash(query) % 10000}"
        
        try:
            # Test crypto price fetching
            from crypto_research import get_price_data
            
            # Extract symbol from query
            symbols = ['BTC', 'ETH', 'SOL', 'MATIC', 'AVAX', 'LINK', 'UNI', 'AAVE', 'COMP', 'MKR']
            symbol = None
            
            for s in symbols:
                if s.lower() in query.lower():
                    symbol = s
                    break
                    
            if not symbol:
                symbol = 'BTC'  # Default
                
            start_time = time.time()
            price_data = await get_price_data(symbol)
            execution_time = time.time() - start_time
            
            if price_data and price_data.get('success'):
                self.record_test_result(test_name, True, f"Price data fetched in {execution_time:.3f}s", category)
            else:
                self.record_test_result(test_name, False, "Failed to fetch price data", category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Exception: {str(e)}", category, traceback.format_exc())
            
    async def test_defi_protocol_query(self, query: str, category: str):
        """Test DeFi protocol queries"""
        test_name = f"defi_{hash(query) % 10000}"
        
        try:
            # Test DeFi protocol data fetching
            from defillama_api import get_protocol_data, search_defi_protocols
            
            # Extract protocol name
            protocols = ['uniswap', 'aave', 'compound', 'makerdao', 'curve', 'balancer', 'sushiswap']
            protocol = None
            
            for p in protocols:
                if p.lower() in query.lower():
                    protocol = p
                    break
                    
            if not protocol:
                protocol = 'uniswap'  # Default
                
            start_time = time.time()
            protocol_data = await get_protocol_data(protocol)
            execution_time = time.time() - start_time
            
            if protocol_data:
                self.record_test_result(test_name, True, f"Protocol data fetched in {execution_time:.3f}s", category)
            else:
                self.record_test_result(test_name, False, "Failed to fetch protocol data", category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Exception: {str(e)}", category, traceback.format_exc())
            
    async def test_portfolio_command(self, command: str, category: str):
        """Test portfolio management commands"""
        test_name = f"portfolio_{hash(command) % 10000}"
        
        try:
            # Test portfolio functionality
            from portfolio_manager import PortfolioManager
            
            portfolio_manager = PortfolioManager()
            user_id = 12345
            
            start_time = time.time()
            
            if "show" in command or "balance" in command:
                result = await portfolio_manager.get_portfolio(user_id)
            elif "add" in command:
                result = await portfolio_manager.add_asset(user_id, "BTC", 1.0)
            elif "remove" in command:
                result = await portfolio_manager.remove_asset(user_id, "ETH")
            else:
                result = {"success": True, "message": "Portfolio command simulated"}
                
            execution_time = time.time() - start_time
            
            if result.get('success', True):
                self.record_test_result(test_name, True, f"Portfolio command executed in {execution_time:.3f}s", category)
            else:
                self.record_test_result(test_name, False, result.get('error', 'Portfolio command failed'), category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Exception: {str(e)}", category, traceback.format_exc())
            
    async def test_alert_command(self, command: str, category: str):
        """Test alert and notification commands"""
        test_name = f"alert_{hash(command) % 10000}"
        
        try:
            # Test alert functionality
            from user_db import add_alert_to_db, get_user_property
            
            user_id = 12345
            
            start_time = time.time()
            
            if "set" in command:
                # Simulate setting an alert
                result = add_alert_to_db(user_id, "BTC", "price", 50000, "above")
            elif "list" in command:
                # Simulate listing alerts
                result = get_user_property(user_id, 'alerts') or []
            else:
                result = True  # Simulate other alert operations
                
            execution_time = time.time() - start_time
            
            if result is not None:
                self.record_test_result(test_name, True, f"Alert command executed in {execution_time:.3f}s", category)
            else:
                self.record_test_result(test_name, False, "Alert command failed", category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Exception: {str(e)}", category, traceback.format_exc())
            
    async def test_ai_provider_command(self, command: str, category: str):
        """Test AI provider switching commands"""
        test_name = f"ai_{hash(command) % 10000}"
        
        try:
            # Test AI provider functionality
            from ai_provider_manager import ai_provider_manager, switch_ai_provider, list_ai_providers
            
            start_time = time.time()
            
            if "switch" in command:
                if "groq" in command:
                    result = switch_ai_provider("groq")
                elif "gemini" in command:
                    result = switch_ai_provider("gemini")
                elif "openai" in command:
                    result = switch_ai_provider("openai")
                else:
                    result = False
            elif "list" in command:
                result = list_ai_providers()
            elif "test" in command:
                result = await ai_provider_manager.test_provider(ai_provider_manager.current_provider)
            else:
                result = ai_provider_manager.get_provider_info()
                
            execution_time = time.time() - start_time
            
            if result:
                self.record_test_result(test_name, True, f"AI provider command executed in {execution_time:.3f}s", category)
            else:
                self.record_test_result(test_name, False, "AI provider command failed", category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Exception: {str(e)}", category, traceback.format_exc())
            
    async def test_error_scenario(self, scenario: str, category: str):
        """Test error handling scenarios"""
        test_name = f"error_{hash(scenario) % 10000}"
        
        try:
            # Test error handling
            from enhanced_natural_language import process_natural_language
            from intelligent_error_handler import error_handler
            
            start_time = time.time()
            
            # Try to process the error scenario
            try:
                result = await process_natural_language(scenario, user_id=12345)
                execution_time = time.time() - start_time
                
                # Error scenarios should be handled gracefully, not crash
                if result:
                    self.record_test_result(test_name, True, f"Error handled gracefully in {execution_time:.3f}s", category)
                else:
                    self.record_test_result(test_name, True, f"Error rejected appropriately in {execution_time:.3f}s", category)
                    
            except Exception as inner_e:
                # If it throws an exception, that's a failure in error handling
                execution_time = time.time() - start_time
                self.record_test_result(test_name, False, f"Unhandled exception: {str(inner_e)}", category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Test setup exception: {str(e)}", category, traceback.format_exc())
            
    async def test_edge_case(self, case: str, category: str):
        """Test edge cases"""
        test_name = f"edge_{hash(str(case)) % 10000}"
        
        try:
            # Test edge case handling
            from enhanced_natural_language import process_natural_language
            
            start_time = time.time()
            
            try:
                result = await process_natural_language(case, user_id=12345)
                execution_time = time.time() - start_time
                
                # Edge cases should be handled without crashing
                self.record_test_result(test_name, True, f"Edge case handled in {execution_time:.3f}s", category)
                
            except Exception as inner_e:
                execution_time = time.time() - start_time
                self.record_test_result(test_name, False, f"Edge case caused exception: {str(inner_e)}", category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Test setup exception: {str(e)}", category, traceback.format_exc())
            
    async def test_performance_scenario(self, test_name: str, query: str, category: str):
        """Test performance scenarios"""
        try:
            from enhanced_natural_language import process_natural_language
            
            # Performance thresholds
            FAST_THRESHOLD = 1.0  # 1 second
            ACCEPTABLE_THRESHOLD = 5.0  # 5 seconds
            
            start_time = time.time()
            
            if "concurrent" in test_name:
                # Test concurrent requests
                tasks = [process_natural_language(query, user_id=12345 + i) for i in range(5)]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                execution_time = time.time() - start_time
                
                success_count = sum(1 for r in results if not isinstance(r, Exception))
                if success_count >= 3:  # At least 3 out of 5 should succeed
                    if execution_time < FAST_THRESHOLD:
                        self.record_test_result(test_name, True, f"Concurrent test completed fast: {execution_time:.3f}s", category)
                    elif execution_time < ACCEPTABLE_THRESHOLD:
                        self.record_test_result(test_name, True, f"Concurrent test completed acceptably: {execution_time:.3f}s", category)
                    else:
                        self.record_test_result(test_name, False, f"Concurrent test too slow: {execution_time:.3f}s", category)
                else:
                    self.record_test_result(test_name, False, f"Too many concurrent failures: {5-success_count}/5", category)
                    
            elif "rapid_fire" in test_name:
                # Test rapid consecutive requests
                for i in range(10):
                    await process_natural_language(f"{query} {i}", user_id=12345)
                    
                execution_time = time.time() - start_time
                
                if execution_time < ACCEPTABLE_THRESHOLD:
                    self.record_test_result(test_name, True, f"Rapid fire test completed: {execution_time:.3f}s", category)
                else:
                    self.record_test_result(test_name, False, f"Rapid fire test too slow: {execution_time:.3f}s", category)
                    
            else:
                # Regular performance test
                result = await process_natural_language(query, user_id=12345)
                execution_time = time.time() - start_time
                
                if execution_time < FAST_THRESHOLD:
                    self.record_test_result(test_name, True, f"Performance test fast: {execution_time:.3f}s", category)
                elif execution_time < ACCEPTABLE_THRESHOLD:
                    self.record_test_result(test_name, True, f"Performance test acceptable: {execution_time:.3f}s", category)
                else:
                    self.record_test_result(test_name, False, f"Performance test slow: {execution_time:.3f}s", category)
                    
        except Exception as e:
            self.record_test_result(test_name, False, f"Performance test exception: {str(e)}", category, traceback.format_exc())
            
    async def test_security_scenario(self, test_name: str, payload: str, category: str):
        """Test security scenarios"""
        try:
            from enhanced_natural_language import process_natural_language
            from input_validator import validate_input
            
            start_time = time.time()
            
            # Test input validation
            is_valid = validate_input(payload)
            
            if not is_valid:
                # Good - security payload was rejected
                execution_time = time.time() - start_time
                self.record_test_result(test_name, True, f"Security payload properly rejected in {execution_time:.3f}s", category)
            else:
                # Test if it's handled safely even if it passes validation
                try:
                    result = await process_natural_language(payload, user_id=12345)
                    execution_time = time.time() - start_time
                    
                    # If it processes without crashing, that's acceptable
                    self.record_test_result(test_name, True, f"Security payload handled safely in {execution_time:.3f}s", category)
                    
                except Exception as inner_e:
                    execution_time = time.time() - start_time
                    # If it crashes, that's a security issue
                    self.record_test_result(test_name, False, f"Security payload caused crash: {str(inner_e)}", category)
                    
        except Exception as e:
            self.record_test_result(test_name, False, f"Security test exception: {str(e)}", category, traceback.format_exc())
            
    async def test_integration_scenario(self, test_name: str, description: str, category: str):
        """Test integration scenarios"""
        try:
            start_time = time.time()
            
            if "coingecko" in test_name:
                from crypto_research import get_price_data
                result = await get_price_data("BTC")
                success = result and result.get('success', False)
                
            elif "defillama" in test_name:
                from defillama_api import get_protocol_data
                result = await get_protocol_data("uniswap")
                success = bool(result)
                
            elif "database" in test_name:
                from user_db import init_db, set_user_property, get_user_property
                init_db()
                set_user_property(12345, 'test_key', 'test_value')
                result = get_user_property(12345, 'test_key')
                success = result == 'test_value'
                
            elif "ai_provider" in test_name:
                from ai_provider_manager import ai_provider_manager
                result = await ai_provider_manager.test_provider(ai_provider_manager.current_provider)
                success = result.get('success', False)
                
            elif "mcp" in test_name:
                from mcp_integration import get_mcp_status
                result = get_mcp_status()
                success = bool(result)
                
            else:
                # Generic integration test
                success = True
                
            execution_time = time.time() - start_time
            
            if success:
                self.record_test_result(test_name, True, f"Integration test passed in {execution_time:.3f}s", category)
            else:
                self.record_test_result(test_name, False, f"Integration test failed", category)
                
        except Exception as e:
            self.record_test_result(test_name, False, f"Integration test exception: {str(e)}", category, traceback.format_exc())
            
    def create_mock_update(self, text: str):
        """Create a mock Telegram update object"""
        update = Mock()
        update.effective_user = Mock()
        update.effective_user.id = 12345
        update.effective_user.username = "test_user"
        update.effective_user.is_bot = False
        
        update.effective_chat = Mock()
        update.effective_chat.id = 12345
        update.effective_chat.type = "private"
        
        update.effective_message = Mock()
        update.effective_message.text = text
        update.effective_message.message_id = 1
        update.effective_message.date = datetime.now()
        update.effective_message.reply_to_message = None
        update.effective_message.reply_text = AsyncMock()
        
        return update
        
    def create_mock_context(self):
        """Create a mock Telegram context object"""
        context = Mock()
        context.bot = Mock()
        context.bot.username = "mobius_bot"
        context.bot_data = {}
        context.args = []
        
        return context
        
    async def simulate_command_execution(self, command: str, update, context):
        """Simulate command execution"""
        try:
            # Import command handlers
            from main import (
                start_command, help_command, status_command, 
                settings_command, profile_command
            )
            
            # Route to appropriate handler
            if command == "/start":
                await start_command(update, context)
            elif command == "/help":
                await help_command(update, context)
            elif command == "/status":
                await status_command(update, context)
            elif command == "/settings":
                await settings_command(update, context)
            elif command == "/profile":
                await profile_command(update, context)
            else:
                # Generic command simulation
                pass
                
            return {"success": True}
            
        except Exception as e:
            return {"success": False, "error": str(e)}
            
    def record_test_result(self, test_name: str, passed: bool, message: str, category: str, traceback_info: str = None):
        """Record a test result"""
        self.total_tests += 1
        
        if passed:
            self.passed_tests += 1
            status = "PASS"
        else:
            self.failed_tests += 1
            status = "FAIL"
            
        result = {
            "test_name": test_name,
            "category": category,
            "status": status,
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "traceback": traceback_info
        }
        
        self.test_results.append(result)
        self.test_categories[category].append(result)
        
        # Log result
        if passed:
            logger.info(f"‚úÖ {test_name}: {message}")
        else:
            logger.error(f"‚ùå {test_name}: {message}")
            if traceback_info:
                logger.error(f"Traceback: {traceback_info}")
                
    async def generate_final_report(self):
        """Generate comprehensive final report"""
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        # Calculate statistics
        pass_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        
        # Category statistics
        category_stats = {}
        for category, results in self.test_categories.items():
            if results:
                passed = sum(1 for r in results if r['status'] == 'PASS')
                total = len(results)
                category_stats[category] = {
                    "total": total,
                    "passed": passed,
                    "failed": total - passed,
                    "pass_rate": (passed / total * 100) if total > 0 else 0
                }
        
        # Generate report
        report = {
            "test_summary": {
                "total_tests": self.total_tests,
                "passed_tests": self.passed_tests,
                "failed_tests": self.failed_tests,
                "pass_rate": pass_rate,
                "start_time": self.start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration.total_seconds()
            },
            "category_statistics": category_stats,
            "detailed_results": self.test_results,
            "failed_tests": [r for r in self.test_results if r['status'] == 'FAIL'],
            "recommendations": self.generate_recommendations()
        }
        
        # Save report
        report_filename = f"comprehensive_coverage_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w') as f:
            json.dump(report, f, indent=2)
            
        # Print summary
        print("\n" + "="*80)
        print("üéØ COMPREHENSIVE COVERAGE TEST REPORT")
        print("="*80)
        print(f"üìä Total Tests: {self.total_tests}")
        print(f"‚úÖ Passed: {self.passed_tests}")
        print(f"‚ùå Failed: {self.failed_tests}")
        print(f"üìà Pass Rate: {pass_rate:.1f}%")
        print(f"‚è±Ô∏è  Duration: {duration.total_seconds():.1f} seconds")
        print(f"üìÑ Report saved to: {report_filename}")
        
        print("\nüìã Category Breakdown:")
        for category, stats in category_stats.items():
            print(f"  {category}: {stats['passed']}/{stats['total']} ({stats['pass_rate']:.1f}%)")
            
        if self.failed_tests > 0:
            print(f"\n‚ùå Failed Tests ({self.failed_tests}):")
            for result in self.test_results:
                if result['status'] == 'FAIL':
                    print(f"  - {result['test_name']}: {result['message']}")
                    
        print("\nüîß Recommendations:")
        for rec in self.generate_recommendations():
            print(f"  ‚Ä¢ {rec}")
            
        print("="*80)
        
        return report
        
    def generate_recommendations(self) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []
        
        # Calculate failure rates by category
        for category, results in self.test_categories.items():
            if results:
                failed = sum(1 for r in results if r['status'] == 'FAIL')
                total = len(results)
                failure_rate = (failed / total * 100) if total > 0 else 0
                
                if failure_rate > 50:
                    recommendations.append(f"Critical: {category} has {failure_rate:.1f}% failure rate - needs immediate attention")
                elif failure_rate > 25:
                    recommendations.append(f"High Priority: {category} has {failure_rate:.1f}% failure rate - should be addressed soon")
                elif failure_rate > 10:
                    recommendations.append(f"Medium Priority: {category} has {failure_rate:.1f}% failure rate - consider improvements")
                    
        # Specific recommendations based on common failure patterns
        failed_tests = [r for r in self.test_results if r['status'] == 'FAIL']
        
        if any('exception' in r['message'].lower() for r in failed_tests):
            recommendations.append("Add more comprehensive exception handling throughout the codebase")
            
        if any('timeout' in r['message'].lower() for r in failed_tests):
            recommendations.append("Implement timeout handling and async optimization for better performance")
            
        if any('api' in r['message'].lower() for r in failed_tests):
            recommendations.append("Improve API error handling and implement fallback mechanisms")
            
        if any('security' in r['category'] for r in failed_tests):
            recommendations.append("Strengthen input validation and security measures")
            
        if any('performance' in r['category'] for r in failed_tests):
            recommendations.append("Optimize performance bottlenecks and implement caching")
            
        # Overall recommendations
        if self.failed_tests > self.passed_tests:
            recommendations.append("Major refactoring needed - more tests failing than passing")
        elif self.failed_tests > 10:
            recommendations.append("Significant improvements needed - focus on most critical failures first")
        elif self.failed_tests > 5:
            recommendations.append("Minor improvements needed - address remaining edge cases")
        else:
            recommendations.append("System is relatively stable - focus on optimization and new features")
            
        return recommendations

async def main():
    """Run the comprehensive coverage test suite"""
    print("üöÄ Starting Comprehensive Coverage Test Suite for M√∂bius AI Assistant")
    print("This will test 50+ different commands and scenarios to identify all bugs and issues")
    print("="*80)
    
    # Initialize test suite
    test_suite = ComprehensiveCoverageTest()
    
    # Run all tests
    await test_suite.run_all_tests()
    
    print("\n‚úÖ Comprehensive Coverage Test Suite completed!")
    print("Check the generated report for detailed results and recommendations.")

if __name__ == "__main__":
    asyncio.run(main())